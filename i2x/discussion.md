# Discussion
i2x’s flexibility makes it suitable for miscellaneous integration use cases, connecting private or public data sources with existing services. More importantly, i2x allows researchers to create their own scenarios, with agents and templates suitable to their work.
The following sections introduce three use cases, using publicly available resources and highlighting i2x’s added value to specific integration and interoperability workflows. In each use case we explain the agent properties, the event that triggers the integration and the respective delivery template.

## Automating variome data integration
Integrating human variome data - collecting unique mutations associated with a specific locus - is a feature made available in several platforms. WAVe [ref WAVe], Cafe Variome [ref CV] or DRUMS [ref ] centralise data from distributed locus-specific databases (LSDB) and make it available through web interfaces.
Despite these systems’ quality, the data integration is limited.  The adopted integration pipeline - extract and curate data, enrich datasets, deliver results - creates a time-based snapshot of the available human variome data.
LOVD provides a turnkey solution to launch new LSDBs, with web and database management interfaces [ref LOVD]. These features make LOVD the most used LSDB, with more than 2 million unique variants stored throughout 78 distinct installations [ref LOVD next].
As mentioned, LOVD has an API enabled by default, which allows obtaining the full list of variants associated with a gene. These are returned in Atom format, a feed specification built in XML. When curators submit new variants to the LOVD instance, the new data becomes available in the feed API.
An i2x agent can be configured to monitor the feed for a single gene or, through the definition of a Seed, for many genes based on a predefined list.
After the initial data population process, new Events are detected when new variants are published. These start a new integration task, which can deliver data directly to the WAVe's database (using a SQL-based template) or send the new variant data to a URL-based service (using a URL route-based template).
This scenario highlights three key benefits of i2x use: autonomy, flexibility and data quality. First, the data warehousing process is triggered autonomously, without user intervention. Next, agents can track any number of genes and deliver data for new variants to any number of heterogeneous templates. At last, variome datasets in the centralised application are always up-to-date with the latest discovered variants. Agent scheduling can monitor every 5 minutes or every week, resulting in improved database completeness.

## Active monitoring DICOM PACS
DICOM PACS architectures are the cornerstone of modern hospital information systems. In these, the integration between patient metadata, clinical notes and medical imaging hardware is fundamental [refs ?].
The management of these systems is a cumbersome task [refs ?], requiring expert personnel and huge financial investments [refs ?]. 
Although i2x can not replace entire DICOM PACS integration infrastructures, it can be used to deploy niche-specific optimisations and to enable the automation of several manual procedures. 
Taking in account the privacy and security concerns associated with these environments, i2x’s distributed implementation comes to play. Hence, we can download and configure an i2x client agent to access data within the secure hospital environment. The client agent can monitor a local Dicoogle platform [refs Dicoogle], which provides multiple search web services, looking for new mammography exams. Finding new exams, triggers a template delivering emails to a list of clinicians. These can include direct URLs to access the medical image for evaluation.

## Publish/subscribe for reference tracking
A rather simple yet helpful scenario involves the tracking of references associated with relevant query terms or, more specifically, interesting identifiers.
For instance, by tracking the list of publications related to the Uniprot P51587 protein, researchers are always up-to-date on the latest studies and developments published covering that particular topic. In this case, the SwissProt knowledge base teams maintains this curated associated references list.
Enabling this scenario in i2x is trivial, requiring the creation of a single integration task, with one agent and one or more templates (one for each notification).
We can also extend the reference tracking publish/subscribe model. To do this, we can configure the UniProt monitor agent to process Seed data during the monitoring process kick-off. The extra requirement is a list of the protein identifiers being tracked. With the configured seed, the UniProt agent URI can be updated to use i2x’s variable scheme to dynamically process protein identifiers, generating unique URLs. For instance, http://uniprot.org/uniprot/%{id}.xml, where *id* is a parameter configured in the seed reader.

## Future Perspectives
Despite its original focus towards the life sciences domain, we can use i2x beyond this scope. Hence, i2x’s future developments roadmap cover several research areas.
i2x is a barebones framework, where the current set of agents and templates support basic features for real-time content monitoring and data delivery. Adding new agents or templates will make i2x more suitable for scenarios outside the life sciences. Integration with social tools, such as Facebook or Twitter, or interoperability with widely-used software, such as Evernote, will increase the number of combinations possible in integrations.
Next, enabling the creation of workflows is also an ongoing effort. The ability to create workflows comprised of more than one integration task will allow deploying more complex ETL solutions within i2x, while maintaining the platform’s simplicity and usability. Adding a new type of delivery template that triggers the execution of an agent detection process can do this.
We will also expand i2x use as a full-featured data warehousing solution. To improve the existing model, we will implement a new component, focused on rule processing. Hence, we can detach the ETL transform task from the delivery templates. This will decouple data transformations to a unique activity within i2x’s workflow. 
In the long-haul, we can further enhance rule processing with the inclusion of semantics. Latest developments on semantic web technologies [refs refs refs] and frameworks [ref coeus] are responsible for an increased adoption within the life sciences field. As such, we plan to include new inference and reasoning engines, allowing researchers to perform more complex operations with their data.
Another planned future research line will deal with the mining of events metadata. Assuming a constant flow of information in and out of i2x, we expect to develop algorithms for the automatic identification of the best schedules for each resource. For instance, if a monitored resource generates events on a daily basis, the system should autonomously understand that    it is efficient to schedule the resource for monitoring every 5 minutes.
At last, we will also focus on creating custom client applications and services based on i2x. As mentioned, i2x is a framework for new applications. Thus, we plan to build various niche-oriented systems, focusing on small tasks that are useful for researchers.


